{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip -q \"/content/drive/MyDrive/dataset.zip\" -d /content/dataset"
      ],
      "metadata": {
        "id": "H5rCY3sjiAtL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3933df6-5f62-4a1c-baeb-d827361ad6a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zSxE-ZdKaCA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e7c0cb-fbdd-4424-dde9-05d6988f0cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ì‚¬ìš© ZIP: /content/drive/MyDrive/dataset.zip\n",
            "â†ªï¸ ë‚´ë¶€ 'dataset' í´ë” ë°œê²¬: í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©\n",
            "ğŸ“ DATASET_DIR: /content/dataset/dataset\n",
            "ì›ë³¸ ë¶„í¬(ê°ì²´):\n",
            " object_class\n",
            "cup_paper      500\n",
            "cup_plastic    500\n",
            "cupholder      500\n",
            "lid            500\n",
            "none           500\n",
            "straw          500\n",
            "Name: count, dtype: int64\n",
            "ì›ë³¸ ë¶„í¬(ì¬ì§ˆ):\n",
            " material_type\n",
            "none        500\n",
            "paper      1000\n",
            "plastic    1500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ê· í˜• ë¶„í¬(ê°ì²´):\n",
            " object_class\n",
            "cup_paper      500\n",
            "cup_plastic    500\n",
            "cupholder      500\n",
            "lid            500\n",
            "none           500\n",
            "straw          500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ê· í˜• ë¶„í¬(ì¬ì§ˆ):\n",
            " material_type\n",
            "none        500\n",
            "paper      1000\n",
            "plastic    1500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "labels.csv ì €ì¥ ì™„ë£Œ: /content/labels.csv (ì´ 3000ì¥)\n",
            "\n",
            "Split -> Train:2100  Val:450  Test:450\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.9803 - material_output_accuracy: 0.8615 - material_output_loss: 0.4132 - object_output_accuracy: 0.8092 - object_output_loss: 0.5671\n",
            "Epoch 1: val_object_output_accuracy improved from -inf to 0.96889, saving model to /content/best_model.keras\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 3s/step - loss: 0.9708 - material_output_accuracy: 0.8629 - material_output_loss: 0.4091 - object_output_accuracy: 0.8111 - object_output_loss: 0.5616 - val_loss: 0.1022 - val_material_output_accuracy: 0.9933 - val_material_output_loss: 0.0199 - val_object_output_accuracy: 0.9689 - val_object_output_loss: 0.0759 - learning_rate: 0.0010\n",
            "Epoch 2/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0339 - material_output_accuracy: 0.9992 - material_output_loss: 0.0053 - object_output_accuracy: 0.9903 - object_output_loss: 0.0286\n",
            "Epoch 2: val_object_output_accuracy improved from 0.96889 to 0.99333, saving model to /content/best_model.keras\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 3s/step - loss: 0.0338 - material_output_accuracy: 0.9992 - material_output_loss: 0.0053 - object_output_accuracy: 0.9904 - object_output_loss: 0.0285 - val_loss: 0.0264 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0082 - val_object_output_accuracy: 0.9933 - val_object_output_loss: 0.0166 - learning_rate: 0.0010\n",
            "Epoch 3/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.0112 - material_output_accuracy: 0.9998 - material_output_loss: 0.0020 - object_output_accuracy: 0.9972 - object_output_loss: 0.0091\n",
            "Epoch 3: val_object_output_accuracy did not improve from 0.99333\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 3s/step - loss: 0.0112 - material_output_accuracy: 0.9998 - material_output_loss: 0.0021 - object_output_accuracy: 0.9972 - object_output_loss: 0.0091 - val_loss: 0.0369 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0035 - val_object_output_accuracy: 0.9911 - val_object_output_loss: 0.0311 - learning_rate: 0.0010\n",
            "Epoch 4/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0105 - material_output_accuracy: 0.9988 - material_output_loss: 0.0044 - object_output_accuracy: 0.9984 - object_output_loss: 0.0062\n",
            "Epoch 4: val_object_output_accuracy did not improve from 0.99333\n",
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 3s/step - loss: 0.0106 - material_output_accuracy: 0.9988 - material_output_loss: 0.0044 - object_output_accuracy: 0.9984 - object_output_loss: 0.0062 - val_loss: 0.0566 - val_material_output_accuracy: 0.9956 - val_material_output_loss: 0.0138 - val_object_output_accuracy: 0.9800 - val_object_output_loss: 0.0393 - learning_rate: 0.0010\n",
            "Epoch 5/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0180 - material_output_accuracy: 0.9959 - material_output_loss: 0.0086 - object_output_accuracy: 0.9977 - object_output_loss: 0.0094\n",
            "Epoch 5: val_object_output_accuracy did not improve from 0.99333\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 3s/step - loss: 0.0180 - material_output_accuracy: 0.9959 - material_output_loss: 0.0085 - object_output_accuracy: 0.9977 - object_output_loss: 0.0094 - val_loss: 0.0339 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0043 - val_object_output_accuracy: 0.9844 - val_object_output_loss: 0.0275 - learning_rate: 4.0000e-04\n",
            "Epoch 6/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0055 - material_output_accuracy: 1.0000 - material_output_loss: 0.0013 - object_output_accuracy: 0.9990 - object_output_loss: 0.0042\n",
            "Epoch 6: val_object_output_accuracy improved from 0.99333 to 0.99778, saving model to /content/best_model.keras\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3s/step - loss: 0.0055 - material_output_accuracy: 1.0000 - material_output_loss: 0.0012 - object_output_accuracy: 0.9990 - object_output_loss: 0.0042 - val_loss: 0.0126 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0034 - val_object_output_accuracy: 0.9978 - val_object_output_loss: 0.0084 - learning_rate: 4.0000e-04\n",
            "Epoch 7/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.0047 - material_output_accuracy: 1.0000 - material_output_loss: 6.8759e-04 - object_output_accuracy: 0.9996 - object_output_loss: 0.0041\n",
            "Epoch 7: val_object_output_accuracy improved from 0.99778 to 1.00000, saving model to /content/best_model.keras\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 3s/step - loss: 0.0048 - material_output_accuracy: 1.0000 - material_output_loss: 7.4964e-04 - object_output_accuracy: 0.9996 - object_output_loss: 0.0041 - val_loss: 0.0081 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0012 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 0.0064 - learning_rate: 4.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Epoch 1/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.8716 - material_output_accuracy: 0.9729 - material_output_loss: 0.1067 - object_output_accuracy: 0.9047 - object_output_loss: 0.7649\n",
            "Epoch 1: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 4s/step - loss: 0.8634 - material_output_accuracy: 0.9731 - material_output_loss: 0.1058 - object_output_accuracy: 0.9054 - object_output_loss: 0.7575 - val_loss: 0.0307 - val_material_output_accuracy: 0.9933 - val_material_output_loss: 0.0136 - val_object_output_accuracy: 0.9933 - val_object_output_loss: 0.0170 - learning_rate: 2.0000e-04\n",
            "Epoch 2/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0245 - material_output_accuracy: 0.9969 - material_output_loss: 0.0097 - object_output_accuracy: 0.9954 - object_output_loss: 0.0148\n",
            "Epoch 2: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 4s/step - loss: 0.0244 - material_output_accuracy: 0.9970 - material_output_loss: 0.0096 - object_output_accuracy: 0.9954 - object_output_loss: 0.0148 - val_loss: 0.0286 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 3.5439e-05 - val_object_output_accuracy: 0.9889 - val_object_output_loss: 0.0268 - learning_rate: 2.0000e-04\n",
            "Epoch 3/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0076 - material_output_accuracy: 1.0000 - material_output_loss: 4.8905e-04 - object_output_accuracy: 0.9993 - object_output_loss: 0.0071\n",
            "Epoch 3: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 3s/step - loss: 0.0076 - material_output_accuracy: 1.0000 - material_output_loss: 4.9321e-04 - object_output_accuracy: 0.9993 - object_output_loss: 0.0071 - val_loss: 0.0028 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0011 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 0.0016 - learning_rate: 2.0000e-04\n",
            "Epoch 4/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0142 - material_output_accuracy: 1.0000 - material_output_loss: 1.7164e-04 - object_output_accuracy: 0.9975 - object_output_loss: 0.0141\n",
            "Epoch 4: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 4s/step - loss: 0.0146 - material_output_accuracy: 1.0000 - material_output_loss: 1.8219e-04 - object_output_accuracy: 0.9974 - object_output_loss: 0.0144 - val_loss: 0.0033 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 5.3879e-04 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 0.0027 - learning_rate: 2.0000e-04\n",
            "Epoch 5/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0152 - material_output_accuracy: 0.9961 - material_output_loss: 0.0087 - object_output_accuracy: 0.9986 - object_output_loss: 0.0065\n",
            "Epoch 5: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4s/step - loss: 0.0152 - material_output_accuracy: 0.9962 - material_output_loss: 0.0087 - object_output_accuracy: 0.9986 - object_output_loss: 0.0065 - val_loss: 7.5118e-04 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 4.0925e-04 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 3.0135e-04 - learning_rate: 2.0000e-04\n",
            "Epoch 6/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0079 - material_output_accuracy: 1.0000 - material_output_loss: 8.7042e-05 - object_output_accuracy: 0.9971 - object_output_loss: 0.0078\n",
            "Epoch 6: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 4s/step - loss: 0.0079 - material_output_accuracy: 1.0000 - material_output_loss: 8.6726e-05 - object_output_accuracy: 0.9971 - object_output_loss: 0.0078 - val_loss: 2.5917e-04 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 5.3728e-06 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 2.3809e-04 - learning_rate: 2.0000e-04\n",
            "Epoch 7/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0112 - material_output_accuracy: 1.0000 - material_output_loss: 6.2207e-05 - object_output_accuracy: 0.9958 - object_output_loss: 0.0112\n",
            "Epoch 7: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 4s/step - loss: 0.0112 - material_output_accuracy: 1.0000 - material_output_loss: 6.2644e-05 - object_output_accuracy: 0.9958 - object_output_loss: 0.0112 - val_loss: 0.0060 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 1.1027e-05 - val_object_output_accuracy: 0.9956 - val_object_output_loss: 0.0056 - learning_rate: 2.0000e-04\n",
            "Epoch 8/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0465 - material_output_accuracy: 0.9955 - material_output_loss: 0.0211 - object_output_accuracy: 0.9957 - object_output_loss: 0.0253\n",
            "Epoch 8: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - loss: 0.0465 - material_output_accuracy: 0.9955 - material_output_loss: 0.0211 - object_output_accuracy: 0.9957 - object_output_loss: 0.0254 - val_loss: 1.1164e-04 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 8.3529e-05 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 2.4310e-05 - learning_rate: 2.0000e-04\n",
            "Epoch 9/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0181 - material_output_accuracy: 0.9984 - material_output_loss: 0.0028 - object_output_accuracy: 0.9942 - object_output_loss: 0.0153\n",
            "Epoch 9: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 4s/step - loss: 0.0181 - material_output_accuracy: 0.9984 - material_output_loss: 0.0028 - object_output_accuracy: 0.9942 - object_output_loss: 0.0153 - val_loss: 0.0032 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 4.4176e-04 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 0.0027 - learning_rate: 2.0000e-04\n",
            "Epoch 10/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0128 - material_output_accuracy: 1.0000 - material_output_loss: 7.2377e-04 - object_output_accuracy: 0.9981 - object_output_loss: 0.0121\n",
            "Epoch 10: val_object_output_accuracy did not improve from 1.00000\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 4s/step - loss: 0.0129 - material_output_accuracy: 1.0000 - material_output_loss: 7.2200e-04 - object_output_accuracy: 0.9981 - object_output_loss: 0.0122 - val_loss: 1.8298e-04 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 2.6350e-05 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 2.0767e-04 - learning_rate: 2.0000e-04\n",
            "Epoch 11/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0020 - material_output_accuracy: 1.0000 - material_output_loss: 2.9118e-04 - object_output_accuracy: 0.9994 - object_output_loss: 0.0017\n",
            "Epoch 11: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 4s/step - loss: 0.0020 - material_output_accuracy: 1.0000 - material_output_loss: 2.9662e-04 - object_output_accuracy: 0.9994 - object_output_loss: 0.0017 - val_loss: 2.4930e-05 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 1.6861e-06 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 2.9687e-05 - learning_rate: 8.0000e-05\n",
            "Epoch 12/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 1.4422e-04 - material_output_accuracy: 1.0000 - material_output_loss: 4.4060e-06 - object_output_accuracy: 1.0000 - object_output_loss: 1.3980e-04\n",
            "Epoch 12: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 4s/step - loss: 1.4458e-04 - material_output_accuracy: 1.0000 - material_output_loss: 4.4639e-06 - object_output_accuracy: 1.0000 - object_output_loss: 1.4009e-04 - val_loss: 6.5637e-06 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 2.2476e-07 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 7.5641e-06 - learning_rate: 8.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "ëª¨ë¸ ì €ì¥: /content/multilabel_model.keras\n",
            "\n",
            "=== [VAL] Object Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cup_paper     1.0000    1.0000    1.0000        75\n",
            " cup_plastic     1.0000    1.0000    1.0000        75\n",
            "   cupholder     1.0000    1.0000    1.0000        75\n",
            "         lid     1.0000    1.0000    1.0000        75\n",
            "       straw     1.0000    1.0000    1.0000        75\n",
            "        none     1.0000    1.0000    1.0000        75\n",
            "\n",
            "    accuracy                         1.0000       450\n",
            "   macro avg     1.0000    1.0000    1.0000       450\n",
            "weighted avg     1.0000    1.0000    1.0000       450\n",
            "\n",
            "Confusion Matrix (Object):\n",
            " [[75  0  0  0  0  0]\n",
            " [ 0 75  0  0  0  0]\n",
            " [ 0  0 75  0  0  0]\n",
            " [ 0  0  0 75  0  0]\n",
            " [ 0  0  0  0 75  0]\n",
            " [ 0  0  0  0  0 75]]\n",
            "\n",
            "=== [VAL] Material Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       paper     1.0000    1.0000    1.0000       150\n",
            "     plastic     1.0000    1.0000    1.0000       225\n",
            "        none     1.0000    1.0000    1.0000        75\n",
            "\n",
            "    accuracy                         1.0000       450\n",
            "   macro avg     1.0000    1.0000    1.0000       450\n",
            "weighted avg     1.0000    1.0000    1.0000       450\n",
            "\n",
            "Confusion Matrix (Material):\n",
            " [[150   0   0]\n",
            " [  0 225   0]\n",
            " [  0   0  75]]\n",
            "\n",
            "=== [TEST] Object Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cup_paper     1.0000    0.9867    0.9933        75\n",
            " cup_plastic     1.0000    1.0000    1.0000        75\n",
            "   cupholder     0.9868    1.0000    0.9934        75\n",
            "         lid     1.0000    1.0000    1.0000        75\n",
            "       straw     1.0000    1.0000    1.0000        75\n",
            "        none     1.0000    1.0000    1.0000        75\n",
            "\n",
            "    accuracy                         0.9978       450\n",
            "   macro avg     0.9978    0.9978    0.9978       450\n",
            "weighted avg     0.9978    0.9978    0.9978       450\n",
            "\n",
            "Confusion Matrix (Object):\n",
            " [[74  0  1  0  0  0]\n",
            " [ 0 75  0  0  0  0]\n",
            " [ 0  0 75  0  0  0]\n",
            " [ 0  0  0 75  0  0]\n",
            " [ 0  0  0  0 75  0]\n",
            " [ 0  0  0  0  0 75]]\n",
            "\n",
            "=== [TEST] Material Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       paper     1.0000    1.0000    1.0000       150\n",
            "     plastic     1.0000    1.0000    1.0000       225\n",
            "        none     1.0000    1.0000    1.0000        75\n",
            "\n",
            "    accuracy                         1.0000       450\n",
            "   macro avg     1.0000    1.0000    1.0000       450\n",
            "weighted avg     1.0000    1.0000    1.0000       450\n",
            "\n",
            "Confusion Matrix (Material):\n",
            " [[150   0   0]\n",
            " [  0 225   0]\n",
            " [  0   0  75]]\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 0) ê¸°ë³¸ ì„¤ì •\n",
        "# =========================================\n",
        "# (í•„ìš” ì‹œ) !pip install --upgrade --force-reinstall pandas scikit-learn\n",
        "\n",
        "import os, glob, zipfile, warnings, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "# í˜¼í•©ì •ë°€ (GPU ìˆì„ ë•Œë§Œ)\n",
        "MIXED = False\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    try:\n",
        "        from tensorflow.keras import mixed_precision\n",
        "        mixed_precision.set_global_policy('mixed_float16')\n",
        "        MIXED = True\n",
        "    except Exception:\n",
        "        MIXED = False\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 0-A) ë°ì´í„°ì…‹ ì¤€ë¹„ (ZIP ì—†ì–´ë„ ë™ì‘)\n",
        "#  - /content/dataset ì•ˆì— cup/ í´ë”ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "#  - í˜„ì¬ ì‘ì—… í´ë”ì—ì„œ ìµœì‹  ZIP ìë™ íƒìƒ‰ (dataset(2).zip ë“± í¬í•¨)\n",
        "#  - ì—†ìœ¼ë©´ ì—…ë¡œë“œ ì°½ í‘œì‹œ(Colab)\n",
        "# =========================================\n",
        "# =========================================\n",
        "# 0-A) ë°ì´í„°ì…‹ ì¤€ë¹„ (ë“œë¼ì´ë¸Œ ZIP ìš°ì„  ì‚¬ìš©)\n",
        "# =========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "EXTRACT_DIR = \"/content/dataset\"\n",
        "# í´ë”ë¡œ ì´ë¯¸ í’€ì–´ë‘” ê²½ìš° ğŸ‘‰ ì´ ê²½ë¡œë¥¼ í´ë”ë¡œ ì§€ì •í•˜ë©´ ì••ì¶•í•´ì œ ìƒëµ\n",
        "CUSTOM_DATASET_DIR = None  # ì˜ˆ: \"/content/drive/MyDrive/dataset\"  (í´ë”ì¼ ë•Œë§Œ)\n",
        "\n",
        "# ë“œë¼ì´ë¸Œì— ì˜¬ë¦° ZIP ê²½ë¡œ (ì •í™•íˆ ì—¬ê¸°ì— ìˆë‹¤ë©´ ë°”ë¡œ ì‚¬ìš©)\n",
        "PREFERRED_ZIP = \"/content/drive/MyDrive/dataset.zip\"\n",
        "\n",
        "def ensure_dataset_dir():\n",
        "    # 1) ì‚¬ìš©ìê°€ í´ë”ë¥¼ ì§ì ‘ ì§€ì •í•œ ê²½ìš° (ZIP ì•„ë‹˜)\n",
        "    if CUSTOM_DATASET_DIR:\n",
        "        assert os.path.isdir(CUSTOM_DATASET_DIR), f\"ê²½ë¡œê°€ í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤: {CUSTOM_DATASET_DIR}\"\n",
        "        print(f\"ì§ì ‘ ì§€ì • í´ë” ì‚¬ìš©: {CUSTOM_DATASET_DIR}\")\n",
        "        return CUSTOM_DATASET_DIR\n",
        "\n",
        "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "    # ì´ë¯¸ í’€ë ¤ ìˆìœ¼ë©´ ì‚¬ìš©\n",
        "    if os.path.isdir(os.path.join(EXTRACT_DIR, \"cup\")):\n",
        "        print(\"ê¸°ì¡´ í´ë” ë°œê²¬: ì••ì¶• í•´ì œ ìŠ¤í‚µ\")\n",
        "        return EXTRACT_DIR\n",
        "\n",
        "    # 2) ë“œë¼ì´ë¸Œ ê³ ì • ê²½ë¡œ ZIP ìš°ì„ \n",
        "    candidate_zips = []\n",
        "    if os.path.isfile(PREFERRED_ZIP):\n",
        "        candidate_zips.append(PREFERRED_ZIP)\n",
        "\n",
        "    # 3) í˜„ì¬ ì‘ì—… í´ë”ì˜ ZIP ìë™ íƒìƒ‰ (dataset(2).zip ë“± í¬í•¨)\n",
        "    candidate_zips += sorted(glob.glob(\"*.zip\"), key=os.path.getmtime, reverse=True)\n",
        "\n",
        "    # 4) ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì—…ë¡œë“œ ìœ ë„\n",
        "    if not candidate_zips:\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            print(\"ğŸ“¤ ZIPì´ ì—†ì–´ ì—…ë¡œë“œ ì°½ì„ ì—½ë‹ˆë‹¤. dataset.zipì„ ì„ íƒí•˜ì„¸ìš”.\")\n",
        "            uploaded = files.upload()\n",
        "            candidate_zips = [os.path.join(os.getcwd(), k) for k in uploaded.keys()]\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(\"*.zip íŒŒì¼ì´ ì—†ê³  ì—…ë¡œë“œë„ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\") from e\n",
        "\n",
        "    zip_path = candidate_zips[0]\n",
        "    print(f\"ì‚¬ìš© ZIP: {zip_path}\")\n",
        "\n",
        "    # ì••ì¶• í…ŒìŠ¤íŠ¸ ë° í•´ì œ\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "            bad = zf.testzip()\n",
        "            if bad:\n",
        "                raise zipfile.BadZipFile(f\"ì†ìƒëœ íŒŒì¼: {bad}\")\n",
        "            zf.extractall(EXTRACT_DIR)\n",
        "    except zipfile.BadZipFile as e:\n",
        "        raise RuntimeError(\n",
        "            f\"ZIPì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤: {e}\\n\"\n",
        "            f\"- í´ë”ë¥¼ ë‹¤ì‹œ ZIPìœ¼ë¡œ ì••ì¶•í•´ ì¬ì—…ë¡œë“œí•˜ê±°ë‚˜,\\n\"\n",
        "            f\"- Google Driveë¥¼ ë§ˆìš´íŠ¸í•´ í´ë” ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •í•˜ì„¸ìš”.\"\n",
        "        )\n",
        "\n",
        "    # ZIP ë‚´ë¶€ì— 'dataset/' í´ë”ê°€ í•œ ê²¹ ë” ìˆìœ¼ë©´ ê·¸ ë‚´ë¶€ë¥¼ ì‚¬ìš©\n",
        "    inner = os.path.join(EXTRACT_DIR, \"dataset\")\n",
        "    if os.path.isdir(inner):\n",
        "        print(\"â†ªï¸ ë‚´ë¶€ 'dataset' í´ë” ë°œê²¬: í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©\")\n",
        "        return inner\n",
        "\n",
        "    return EXTRACT_DIR\n",
        "\n",
        "DATASET_DIR = ensure_dataset_dir()\n",
        "print(\"ğŸ“ DATASET_DIR:\", DATASET_DIR)\n",
        "\n",
        "# =========================================\n",
        "# 1) ë¼ë²¨ ê·œì¹™ (ê°ì²´ 6í´ë˜ìŠ¤ + ì¬ì§ˆ 3í´ë˜ìŠ¤)\n",
        "#  - ìš”ì²­í•˜ì‹  ë§¤í•‘ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜\n",
        "# =========================================\n",
        "OBJECT_CLASS_NAMES = ['cup_paper', 'cup_plastic', 'cupholder', 'lid', 'straw', 'none']\n",
        "OBJECT_CLASS_MAP   = {n:i for i,n in enumerate(OBJECT_CLASS_NAMES)}\n",
        "\n",
        "MATERIAL_CLASS_NAMES= ['paper', 'plastic', 'none']\n",
        "MATERIAL_CLASS_MAP  = {n:i for i,n in enumerate(MATERIAL_CLASS_NAMES)}\n",
        "\n",
        "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
        "\n",
        "def infer_labels_from_path(path: str):\n",
        "    \"\"\"\n",
        "    í´ë” êµ¬ì¡°:\n",
        "      - cup/paper/*, cup/plastic/*\n",
        "      - cupholder/*\n",
        "      - lid/*\n",
        "      - straw/*\n",
        "      - none/*\n",
        "    ì¬ì§ˆ ë§¤í•‘:\n",
        "      cup_paper â†’ paper\n",
        "      cup_plastic â†’ plastic\n",
        "      cupholder â†’ paper\n",
        "      lid â†’ plastic\n",
        "      straw â†’ plastic\n",
        "      none â†’ none\n",
        "    \"\"\"\n",
        "    base = Path(DATASET_DIR).resolve()\n",
        "    rel  = Path(path).resolve().relative_to(base)\n",
        "    top  = rel.parts[0].lower()\n",
        "    sub  = rel.parts[1].lower() if len(rel.parts) > 1 else ''\n",
        "\n",
        "    if top == 'cup':\n",
        "        if sub == 'paper':\n",
        "            return ('cup_paper', 'paper')\n",
        "        elif sub == 'plastic':\n",
        "            return ('cup_plastic', 'plastic')\n",
        "        else:\n",
        "            # cup ë°”ë¡œ ì•„ë˜ ì´ë¯¸ì§€ì¸ ê²½ìš° íŒŒì¼ëª… íŒíŠ¸\n",
        "            name = Path(path).stem.lower()\n",
        "            if 'paper' in name:\n",
        "                return ('cup_paper', 'paper')\n",
        "            elif 'plastic' in name:\n",
        "                return ('cup_plastic', 'plastic')\n",
        "            raise ValueError(f\"[cup] ì¬ì§ˆ í•˜ìœ„í´ë”/íŒíŠ¸ ë¶ˆëª…í™•: {path}\")\n",
        "\n",
        "    elif top == 'cupholder':\n",
        "        return ('cupholder', 'paper')\n",
        "\n",
        "    elif top == 'lid':\n",
        "        return ('lid', 'plastic')\n",
        "\n",
        "    elif top == 'straw':\n",
        "        return ('straw', 'plastic')  # ì „ë¶€ plastic ê³ ì •\n",
        "\n",
        "    elif top == 'none':\n",
        "        return ('none', 'none')\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"ì•Œ ìˆ˜ ì—†ëŠ” ìµœìƒìœ„ í´ë”: {top} (path={path})\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 2) CSV ìƒì„±\n",
        "# =========================================\n",
        "image_paths, object_classes, materials = [], [], []\n",
        "\n",
        "for root, _, files in os.walk(DATASET_DIR):\n",
        "    for fname in files:\n",
        "        if fname.lower().endswith(IMG_EXTS):\n",
        "            fpath = os.path.join(root, fname)\n",
        "            try:\n",
        "                obj_cls, mat = infer_labels_from_path(fpath)\n",
        "            except Exception as e:\n",
        "                print(\"ë¼ë²¨ ì¶”ë¡  ìŠ¤í‚µ:\", e)\n",
        "                continue\n",
        "            image_paths.append(fpath)\n",
        "            object_classes.append(obj_cls)\n",
        "            materials.append(mat)\n",
        "\n",
        "if not image_paths:\n",
        "    raise RuntimeError(\"ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í´ë” êµ¬ì¡°/í™•ì¥ì í™•ì¸ í•„ìš”.\")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"image_path\": image_paths,\n",
        "    \"object_class\": object_classes,\n",
        "    \"material_type\": materials\n",
        "})\n",
        "\n",
        "print(\"ì›ë³¸ ë¶„í¬(ê°ì²´):\\n\", df['object_class'].value_counts().sort_index())\n",
        "print(\"ì›ë³¸ ë¶„í¬(ì¬ì§ˆ):\\n\", df['material_type'].value_counts().sort_index())\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 3) ê· í˜•í™” (ê°ì²´ í´ë˜ìŠ¤ë³„ ì •í™•íˆ 500ì¥)\n",
        "# =========================================\n",
        "TARGET_PER_CLASS = 500\n",
        "balanced = []\n",
        "for cls in OBJECT_CLASS_NAMES:\n",
        "    g = df[df['object_class'] == cls]\n",
        "    n = len(g)\n",
        "    if n == 0:\n",
        "        raise RuntimeError(f\"'{cls}' í´ë˜ìŠ¤ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    if n == TARGET_PER_CLASS:\n",
        "        balanced.append(g)\n",
        "    elif n > TARGET_PER_CLASS:\n",
        "        balanced.append(g.sample(n=TARGET_PER_CLASS, replace=False, random_state=SEED))\n",
        "    else:\n",
        "        balanced.append(g.sample(n=TARGET_PER_CLASS, replace=True, random_state=SEED))\n",
        "\n",
        "df = pd.concat(balanced, ignore_index=True)\n",
        "\n",
        "# ë¼ë²¨ ì¸ì½”ë”©\n",
        "df['object_label']   = df['object_class'].map(OBJECT_CLASS_MAP).astype(np.int32)\n",
        "df['material_label'] = df['material_type'].map(MATERIAL_CLASS_MAP).astype(np.int32)\n",
        "\n",
        "print(\"\\nê· í˜• ë¶„í¬(ê°ì²´):\\n\", df['object_class'].value_counts().sort_index())\n",
        "print(\"\\nê· í˜• ë¶„í¬(ì¬ì§ˆ):\\n\", df['material_type'].value_counts().sort_index())\n",
        "\n",
        "CSV_PATH = \"/content/labels.csv\"\n",
        "df.to_csv(CSV_PATH, index=False)\n",
        "print(f\"\\nlabels.csv ì €ì¥ ì™„ë£Œ: {CSV_PATH} (ì´ {len(df)}ì¥)\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 4) ë°ì´í„° ë¶„í•  (Train/Val/Test = 70/15/15, ê°ì²´ ê¸°ì¤€ stratify)\n",
        "# =========================================\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.30, random_state=SEED, stratify=df['object_label']\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.50, random_state=SEED, stratify=temp_df['object_label']\n",
        ")\n",
        "\n",
        "print(f\"\\nSplit -> Train:{len(train_df)}  Val:{len(val_df)}  Test:{len(test_df)}\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 5) tf.data íŒŒì´í”„ë¼ì¸\n",
        "# =========================================\n",
        "IMG_SIZE = (180, 180)\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "], name=\"augmentation\")\n",
        "\n",
        "def load_and_preprocess(path, obj_label, mat_label, augment=False):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, IMG_SIZE, method='bilinear')\n",
        "    img = preprocess_input(img)  # [-1,1]\n",
        "    if augment:\n",
        "        img = data_augmentation(img, training=True)\n",
        "    return img, {'object_output': obj_label, 'material_output': mat_label}\n",
        "\n",
        "def make_ds(dataframe, training=False):\n",
        "    paths = dataframe['image_path'].astype(str).values\n",
        "    obj   = dataframe['object_label'].astype(np.int32).values\n",
        "    mat   = dataframe['material_label'].astype(np.int32).values\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, obj, mat))\n",
        "    if training:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe), seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(lambda p,o,m: load_and_preprocess(p,o,m,augment=training), num_parallel_calls=AUTO)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
        "    # ê²€ì¦/í…ŒìŠ¤íŠ¸ëŠ” ìˆœì„œ ë³´ì¥(í‰ê°€ ì‹ ë¢°ë„ â†‘)\n",
        "    if not training:\n",
        "        opts = tf.data.Options()\n",
        "        opts.experimental_deterministic = True\n",
        "        ds = ds.with_options(opts)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_ds(train_df, training=True)\n",
        "val_ds   = make_ds(val_df,   training=False)\n",
        "test_ds  = make_ds(test_df,  training=False)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 6) ëª¨ë¸ (MobileNetV2 ë©€í‹°ì•„ì›ƒí’‹)\n",
        "# =========================================\n",
        "def build_model():\n",
        "    base = MobileNetV2(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), include_top=False, weights='imagenet')\n",
        "    base.trainable = False\n",
        "\n",
        "    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    x = base(inp, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    dtype_out = 'float32' if MIXED else None\n",
        "    object_output   = layers.Dense(len(OBJECT_CLASS_NAMES),   activation='softmax', dtype=dtype_out, name='object_output')(x)\n",
        "    material_output = layers.Dense(len(MATERIAL_CLASS_NAMES), activation='softmax', dtype=dtype_out, name='material_output')(x)\n",
        "\n",
        "    model = models.Model(inp, {'object_output': object_output, 'material_output': material_output})\n",
        "    return model, base\n",
        "\n",
        "model, base = build_model()\n",
        "\n",
        "ckpt_path = \"/content/best_model.keras\"\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_object_output_accuracy',\n",
        "                                       mode='max', save_best_only=True, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=2, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# 1ë‹¨ê³„: Warm-up (ë² ì´ìŠ¤ ê³ ì •)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss={'object_output': 'sparse_categorical_crossentropy',\n",
        "          'material_output': 'sparse_categorical_crossentropy'},\n",
        "    metrics={'object_output': 'accuracy', 'material_output': 'accuracy'}\n",
        ")\n",
        "history_warm = model.fit(train_ds, validation_data=val_ds, epochs=7, callbacks=callbacks, verbose=1)\n",
        "\n",
        "# 2ë‹¨ê³„: ìƒìœ„ ë¸”ë¡ íŒŒì¸íŠœë‹ (BN ì œì™¸)\n",
        "N_TRAIN = 50\n",
        "for lyr in base.layers[-N_TRAIN:]:\n",
        "    if not isinstance(lyr, layers.BatchNormalization):\n",
        "        lyr.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(2e-4),\n",
        "    loss={'object_output': 'sparse_categorical_crossentropy',\n",
        "          'material_output': 'sparse_categorical_crossentropy'},\n",
        "    metrics={'object_output': 'accuracy', 'material_output': 'accuracy'}\n",
        ")\n",
        "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=12, callbacks=callbacks, verbose=1)\n",
        "\n",
        "FINAL_MODEL_PATH = \"/content/multilabel_model.keras\"\n",
        "model.save(FINAL_MODEL_PATH)\n",
        "print(\"ëª¨ë¸ ì €ì¥:\", FINAL_MODEL_PATH)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 7) í‰ê°€ (Val/Test) - ê°ì²´/ì¬ì§ˆ\n",
        "#  - val/testëŠ” ìˆœì„œë¥¼ ê³ ì •í–ˆìœ¼ë¯€ë¡œ dfì™€ 1:1 ëŒ€ì‘\n",
        "# =========================================\n",
        "def evaluate_split(name, ds, df_ref):\n",
        "    preds = model.predict(ds, verbose=0)\n",
        "    y_obj_pred = preds['object_output'].argmax(axis=1)\n",
        "    y_mat_pred = preds['material_output'].argmax(axis=1)\n",
        "\n",
        "    y_obj_true = df_ref['object_label'].values\n",
        "    y_mat_true = df_ref['material_label'].values\n",
        "\n",
        "    print(f\"\\n=== [{name}] Object Report ===\")\n",
        "    print(classification_report(y_obj_true, y_obj_pred, target_names=OBJECT_CLASS_NAMES, digits=4))\n",
        "    print(\"Confusion Matrix (Object):\\n\", confusion_matrix(y_obj_true, y_obj_pred))\n",
        "\n",
        "    print(f\"\\n=== [{name}] Material Report ===\")\n",
        "    print(classification_report(y_mat_true, y_mat_pred, target_names=MATERIAL_CLASS_NAMES, digits=4))\n",
        "    print(\"Confusion Matrix (Material):\\n\", confusion_matrix(y_mat_true, y_mat_pred))\n",
        "\n",
        "evaluate_split(\"VAL\",  val_ds,  val_df)\n",
        "evaluate_split(\"TEST\", test_ds, test_df)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 8) ì¶”ë¡  ìœ í‹¸\n",
        "# =========================================\n",
        "from tensorflow.keras.preprocessing import image as kimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_image(img_path, none_threshold=0.55):\n",
        "    p = Path(img_path)\n",
        "    if not p.exists():\n",
        "        print(f\"íŒŒì¼ ì—†ìŒ: {img_path}\"); return\n",
        "    img = kimage.load_img(img_path, target_size=IMG_SIZE)\n",
        "    arr = kimage.img_to_array(img)\n",
        "    x = tf.expand_dims(preprocess_input(arr), axis=0)\n",
        "\n",
        "    preds = model.predict(x, verbose=0)\n",
        "    obj_prob = preds['object_output'][0]\n",
        "    mat_prob = preds['material_output'][0]\n",
        "    o_idx = int(np.argmax(obj_prob))\n",
        "    m_idx = int(np.argmax(mat_prob))\n",
        "\n",
        "    # ë‚®ì€ í™•ë¥ ì´ë©´ noneìœ¼ë¡œ ê²Œì´íŒ… (none ìì²´ê°€ ìµœê³ ë©´ ìœ ì§€)\n",
        "    if obj_prob[o_idx] < none_threshold and OBJECT_CLASS_NAMES[o_idx] != 'none':\n",
        "        o_idx = OBJECT_CLASS_MAP['none']\n",
        "\n",
        "    print(f\"Object: {OBJECT_CLASS_NAMES[o_idx]} ({obj_prob[o_idx]*100:.2f}%)\")\n",
        "    print(f\"Material: {MATERIAL_CLASS_NAMES[m_idx]} ({mat_prob[m_idx]*100:.2f}%)\")\n",
        "\n",
        "def predict_and_show(img_path, none_threshold=0.55):\n",
        "    img = kimage.load_img(img_path, target_size=IMG_SIZE)\n",
        "    arr = kimage.img_to_array(img).astype(\"uint8\")\n",
        "    x = tf.expand_dims(preprocess_input(arr), axis=0)\n",
        "\n",
        "    preds = model.predict(x, verbose=0)\n",
        "    obj_prob = preds['object_output'][0]\n",
        "    mat_prob = preds['material_output'][0]\n",
        "    o_idx = int(np.argmax(obj_prob))\n",
        "    m_idx = int(np.argmax(mat_prob))\n",
        "    if obj_prob[o_idx] < none_threshold and OBJECT_CLASS_NAMES[o_idx] != 'none':\n",
        "        o_idx = OBJECT_CLASS_MAP['none']\n",
        "\n",
        "    plt.imshow(arr); plt.axis(\"off\")\n",
        "    plt.title(f\"{OBJECT_CLASS_NAMES[o_idx]} ({obj_prob[o_idx]*100:.1f}%)\\n\"\n",
        "              f\"{MATERIAL_CLASS_NAMES[m_idx]} ({mat_prob[m_idx]*100:.1f}%)\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KQW7U4h9HDem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3a83e7-986b-4547-e1ab-ce51fe036f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object_class  material_type\n",
            "cup_paper     paper            500\n",
            "cup_plastic   plastic          500\n",
            "cupholder     paper            500\n",
            "lid           plastic          500\n",
            "none          none             500\n",
            "straw         plastic          500\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/labels.csv\")\n",
        "print(df.groupby(['object_class','material_type']).size().sort_index())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}