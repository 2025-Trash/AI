{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip -q \"/content/drive/MyDrive/dataset.zip\" -d /content/dataset"
      ],
      "metadata": {
        "id": "H5rCY3sjiAtL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3933df6-5f62-4a1c-baeb-d827361ad6a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zSxE-ZdKaCA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e7c0cb-fbdd-4424-dde9-05d6988f0cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "사용 ZIP: /content/drive/MyDrive/dataset.zip\n",
            "↪️ 내부 'dataset' 폴더 발견: 해당 경로 사용\n",
            "📁 DATASET_DIR: /content/dataset/dataset\n",
            "원본 분포(객체):\n",
            " object_class\n",
            "cup_paper      500\n",
            "cup_plastic    500\n",
            "cupholder      500\n",
            "lid            500\n",
            "none           500\n",
            "straw          500\n",
            "Name: count, dtype: int64\n",
            "원본 분포(재질):\n",
            " material_type\n",
            "none        500\n",
            "paper      1000\n",
            "plastic    1500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "균형 분포(객체):\n",
            " object_class\n",
            "cup_paper      500\n",
            "cup_plastic    500\n",
            "cupholder      500\n",
            "lid            500\n",
            "none           500\n",
            "straw          500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "균형 분포(재질):\n",
            " material_type\n",
            "none        500\n",
            "paper      1000\n",
            "plastic    1500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "labels.csv 저장 완료: /content/labels.csv (총 3000장)\n",
            "\n",
            "Split -> Train:2100  Val:450  Test:450\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.9803 - material_output_accuracy: 0.8615 - material_output_loss: 0.4132 - object_output_accuracy: 0.8092 - object_output_loss: 0.5671\n",
            "Epoch 1: val_object_output_accuracy improved from -inf to 0.96889, saving model to /content/best_model.keras\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 3s/step - loss: 0.9708 - material_output_accuracy: 0.8629 - material_output_loss: 0.4091 - object_output_accuracy: 0.8111 - object_output_loss: 0.5616 - val_loss: 0.1022 - val_material_output_accuracy: 0.9933 - val_material_output_loss: 0.0199 - val_object_output_accuracy: 0.9689 - val_object_output_loss: 0.0759 - learning_rate: 0.0010\n",
            "Epoch 2/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0339 - material_output_accuracy: 0.9992 - material_output_loss: 0.0053 - object_output_accuracy: 0.9903 - object_output_loss: 0.0286\n",
            "Epoch 2: val_object_output_accuracy improved from 0.96889 to 0.99333, saving model to /content/best_model.keras\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 3s/step - loss: 0.0338 - material_output_accuracy: 0.9992 - material_output_loss: 0.0053 - object_output_accuracy: 0.9904 - object_output_loss: 0.0285 - val_loss: 0.0264 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0082 - val_object_output_accuracy: 0.9933 - val_object_output_loss: 0.0166 - learning_rate: 0.0010\n",
            "Epoch 3/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.0112 - material_output_accuracy: 0.9998 - material_output_loss: 0.0020 - object_output_accuracy: 0.9972 - object_output_loss: 0.0091\n",
            "Epoch 3: val_object_output_accuracy did not improve from 0.99333\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 3s/step - loss: 0.0112 - material_output_accuracy: 0.9998 - material_output_loss: 0.0021 - object_output_accuracy: 0.9972 - object_output_loss: 0.0091 - val_loss: 0.0369 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0035 - val_object_output_accuracy: 0.9911 - val_object_output_loss: 0.0311 - learning_rate: 0.0010\n",
            "Epoch 4/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0105 - material_output_accuracy: 0.9988 - material_output_loss: 0.0044 - object_output_accuracy: 0.9984 - object_output_loss: 0.0062\n",
            "Epoch 4: val_object_output_accuracy did not improve from 0.99333\n",
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 3s/step - loss: 0.0106 - material_output_accuracy: 0.9988 - material_output_loss: 0.0044 - object_output_accuracy: 0.9984 - object_output_loss: 0.0062 - val_loss: 0.0566 - val_material_output_accuracy: 0.9956 - val_material_output_loss: 0.0138 - val_object_output_accuracy: 0.9800 - val_object_output_loss: 0.0393 - learning_rate: 0.0010\n",
            "Epoch 5/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0180 - material_output_accuracy: 0.9959 - material_output_loss: 0.0086 - object_output_accuracy: 0.9977 - object_output_loss: 0.0094\n",
            "Epoch 5: val_object_output_accuracy did not improve from 0.99333\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 3s/step - loss: 0.0180 - material_output_accuracy: 0.9959 - material_output_loss: 0.0085 - object_output_accuracy: 0.9977 - object_output_loss: 0.0094 - val_loss: 0.0339 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0043 - val_object_output_accuracy: 0.9844 - val_object_output_loss: 0.0275 - learning_rate: 4.0000e-04\n",
            "Epoch 6/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0055 - material_output_accuracy: 1.0000 - material_output_loss: 0.0013 - object_output_accuracy: 0.9990 - object_output_loss: 0.0042\n",
            "Epoch 6: val_object_output_accuracy improved from 0.99333 to 0.99778, saving model to /content/best_model.keras\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3s/step - loss: 0.0055 - material_output_accuracy: 1.0000 - material_output_loss: 0.0012 - object_output_accuracy: 0.9990 - object_output_loss: 0.0042 - val_loss: 0.0126 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0034 - val_object_output_accuracy: 0.9978 - val_object_output_loss: 0.0084 - learning_rate: 4.0000e-04\n",
            "Epoch 7/7\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.0047 - material_output_accuracy: 1.0000 - material_output_loss: 6.8759e-04 - object_output_accuracy: 0.9996 - object_output_loss: 0.0041\n",
            "Epoch 7: val_object_output_accuracy improved from 0.99778 to 1.00000, saving model to /content/best_model.keras\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 3s/step - loss: 0.0048 - material_output_accuracy: 1.0000 - material_output_loss: 7.4964e-04 - object_output_accuracy: 0.9996 - object_output_loss: 0.0041 - val_loss: 0.0081 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0012 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 0.0064 - learning_rate: 4.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Epoch 1/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.8716 - material_output_accuracy: 0.9729 - material_output_loss: 0.1067 - object_output_accuracy: 0.9047 - object_output_loss: 0.7649\n",
            "Epoch 1: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 4s/step - loss: 0.8634 - material_output_accuracy: 0.9731 - material_output_loss: 0.1058 - object_output_accuracy: 0.9054 - object_output_loss: 0.7575 - val_loss: 0.0307 - val_material_output_accuracy: 0.9933 - val_material_output_loss: 0.0136 - val_object_output_accuracy: 0.9933 - val_object_output_loss: 0.0170 - learning_rate: 2.0000e-04\n",
            "Epoch 2/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0245 - material_output_accuracy: 0.9969 - material_output_loss: 0.0097 - object_output_accuracy: 0.9954 - object_output_loss: 0.0148\n",
            "Epoch 2: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 4s/step - loss: 0.0244 - material_output_accuracy: 0.9970 - material_output_loss: 0.0096 - object_output_accuracy: 0.9954 - object_output_loss: 0.0148 - val_loss: 0.0286 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 3.5439e-05 - val_object_output_accuracy: 0.9889 - val_object_output_loss: 0.0268 - learning_rate: 2.0000e-04\n",
            "Epoch 3/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0076 - material_output_accuracy: 1.0000 - material_output_loss: 4.8905e-04 - object_output_accuracy: 0.9993 - object_output_loss: 0.0071\n",
            "Epoch 3: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 3s/step - loss: 0.0076 - material_output_accuracy: 1.0000 - material_output_loss: 4.9321e-04 - object_output_accuracy: 0.9993 - object_output_loss: 0.0071 - val_loss: 0.0028 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 0.0011 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 0.0016 - learning_rate: 2.0000e-04\n",
            "Epoch 4/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0142 - material_output_accuracy: 1.0000 - material_output_loss: 1.7164e-04 - object_output_accuracy: 0.9975 - object_output_loss: 0.0141\n",
            "Epoch 4: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 4s/step - loss: 0.0146 - material_output_accuracy: 1.0000 - material_output_loss: 1.8219e-04 - object_output_accuracy: 0.9974 - object_output_loss: 0.0144 - val_loss: 0.0033 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 5.3879e-04 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 0.0027 - learning_rate: 2.0000e-04\n",
            "Epoch 5/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0152 - material_output_accuracy: 0.9961 - material_output_loss: 0.0087 - object_output_accuracy: 0.9986 - object_output_loss: 0.0065\n",
            "Epoch 5: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4s/step - loss: 0.0152 - material_output_accuracy: 0.9962 - material_output_loss: 0.0087 - object_output_accuracy: 0.9986 - object_output_loss: 0.0065 - val_loss: 7.5118e-04 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 4.0925e-04 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 3.0135e-04 - learning_rate: 2.0000e-04\n",
            "Epoch 6/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0079 - material_output_accuracy: 1.0000 - material_output_loss: 8.7042e-05 - object_output_accuracy: 0.9971 - object_output_loss: 0.0078\n",
            "Epoch 6: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 4s/step - loss: 0.0079 - material_output_accuracy: 1.0000 - material_output_loss: 8.6726e-05 - object_output_accuracy: 0.9971 - object_output_loss: 0.0078 - val_loss: 2.5917e-04 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 5.3728e-06 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 2.3809e-04 - learning_rate: 2.0000e-04\n",
            "Epoch 7/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0112 - material_output_accuracy: 1.0000 - material_output_loss: 6.2207e-05 - object_output_accuracy: 0.9958 - object_output_loss: 0.0112\n",
            "Epoch 7: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 4s/step - loss: 0.0112 - material_output_accuracy: 1.0000 - material_output_loss: 6.2644e-05 - object_output_accuracy: 0.9958 - object_output_loss: 0.0112 - val_loss: 0.0060 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 1.1027e-05 - val_object_output_accuracy: 0.9956 - val_object_output_loss: 0.0056 - learning_rate: 2.0000e-04\n",
            "Epoch 8/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0465 - material_output_accuracy: 0.9955 - material_output_loss: 0.0211 - object_output_accuracy: 0.9957 - object_output_loss: 0.0253\n",
            "Epoch 8: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - loss: 0.0465 - material_output_accuracy: 0.9955 - material_output_loss: 0.0211 - object_output_accuracy: 0.9957 - object_output_loss: 0.0254 - val_loss: 1.1164e-04 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 8.3529e-05 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 2.4310e-05 - learning_rate: 2.0000e-04\n",
            "Epoch 9/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0181 - material_output_accuracy: 0.9984 - material_output_loss: 0.0028 - object_output_accuracy: 0.9942 - object_output_loss: 0.0153\n",
            "Epoch 9: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 4s/step - loss: 0.0181 - material_output_accuracy: 0.9984 - material_output_loss: 0.0028 - object_output_accuracy: 0.9942 - object_output_loss: 0.0153 - val_loss: 0.0032 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 4.4176e-04 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 0.0027 - learning_rate: 2.0000e-04\n",
            "Epoch 10/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0128 - material_output_accuracy: 1.0000 - material_output_loss: 7.2377e-04 - object_output_accuracy: 0.9981 - object_output_loss: 0.0121\n",
            "Epoch 10: val_object_output_accuracy did not improve from 1.00000\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 4s/step - loss: 0.0129 - material_output_accuracy: 1.0000 - material_output_loss: 7.2200e-04 - object_output_accuracy: 0.9981 - object_output_loss: 0.0122 - val_loss: 1.8298e-04 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 2.6350e-05 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 2.0767e-04 - learning_rate: 2.0000e-04\n",
            "Epoch 11/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0020 - material_output_accuracy: 1.0000 - material_output_loss: 2.9118e-04 - object_output_accuracy: 0.9994 - object_output_loss: 0.0017\n",
            "Epoch 11: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 4s/step - loss: 0.0020 - material_output_accuracy: 1.0000 - material_output_loss: 2.9662e-04 - object_output_accuracy: 0.9994 - object_output_loss: 0.0017 - val_loss: 2.4930e-05 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 1.6861e-06 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 2.9687e-05 - learning_rate: 8.0000e-05\n",
            "Epoch 12/12\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 1.4422e-04 - material_output_accuracy: 1.0000 - material_output_loss: 4.4060e-06 - object_output_accuracy: 1.0000 - object_output_loss: 1.3980e-04\n",
            "Epoch 12: val_object_output_accuracy did not improve from 1.00000\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 4s/step - loss: 1.4458e-04 - material_output_accuracy: 1.0000 - material_output_loss: 4.4639e-06 - object_output_accuracy: 1.0000 - object_output_loss: 1.4009e-04 - val_loss: 6.5637e-06 - val_material_output_accuracy: 1.0000 - val_material_output_loss: 2.2476e-07 - val_object_output_accuracy: 1.0000 - val_object_output_loss: 7.5641e-06 - learning_rate: 8.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "모델 저장: /content/multilabel_model.keras\n",
            "\n",
            "=== [VAL] Object Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cup_paper     1.0000    1.0000    1.0000        75\n",
            " cup_plastic     1.0000    1.0000    1.0000        75\n",
            "   cupholder     1.0000    1.0000    1.0000        75\n",
            "         lid     1.0000    1.0000    1.0000        75\n",
            "       straw     1.0000    1.0000    1.0000        75\n",
            "        none     1.0000    1.0000    1.0000        75\n",
            "\n",
            "    accuracy                         1.0000       450\n",
            "   macro avg     1.0000    1.0000    1.0000       450\n",
            "weighted avg     1.0000    1.0000    1.0000       450\n",
            "\n",
            "Confusion Matrix (Object):\n",
            " [[75  0  0  0  0  0]\n",
            " [ 0 75  0  0  0  0]\n",
            " [ 0  0 75  0  0  0]\n",
            " [ 0  0  0 75  0  0]\n",
            " [ 0  0  0  0 75  0]\n",
            " [ 0  0  0  0  0 75]]\n",
            "\n",
            "=== [VAL] Material Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       paper     1.0000    1.0000    1.0000       150\n",
            "     plastic     1.0000    1.0000    1.0000       225\n",
            "        none     1.0000    1.0000    1.0000        75\n",
            "\n",
            "    accuracy                         1.0000       450\n",
            "   macro avg     1.0000    1.0000    1.0000       450\n",
            "weighted avg     1.0000    1.0000    1.0000       450\n",
            "\n",
            "Confusion Matrix (Material):\n",
            " [[150   0   0]\n",
            " [  0 225   0]\n",
            " [  0   0  75]]\n",
            "\n",
            "=== [TEST] Object Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cup_paper     1.0000    0.9867    0.9933        75\n",
            " cup_plastic     1.0000    1.0000    1.0000        75\n",
            "   cupholder     0.9868    1.0000    0.9934        75\n",
            "         lid     1.0000    1.0000    1.0000        75\n",
            "       straw     1.0000    1.0000    1.0000        75\n",
            "        none     1.0000    1.0000    1.0000        75\n",
            "\n",
            "    accuracy                         0.9978       450\n",
            "   macro avg     0.9978    0.9978    0.9978       450\n",
            "weighted avg     0.9978    0.9978    0.9978       450\n",
            "\n",
            "Confusion Matrix (Object):\n",
            " [[74  0  1  0  0  0]\n",
            " [ 0 75  0  0  0  0]\n",
            " [ 0  0 75  0  0  0]\n",
            " [ 0  0  0 75  0  0]\n",
            " [ 0  0  0  0 75  0]\n",
            " [ 0  0  0  0  0 75]]\n",
            "\n",
            "=== [TEST] Material Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       paper     1.0000    1.0000    1.0000       150\n",
            "     plastic     1.0000    1.0000    1.0000       225\n",
            "        none     1.0000    1.0000    1.0000        75\n",
            "\n",
            "    accuracy                         1.0000       450\n",
            "   macro avg     1.0000    1.0000    1.0000       450\n",
            "weighted avg     1.0000    1.0000    1.0000       450\n",
            "\n",
            "Confusion Matrix (Material):\n",
            " [[150   0   0]\n",
            " [  0 225   0]\n",
            " [  0   0  75]]\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 0) 기본 설정\n",
        "# =========================================\n",
        "# (필요 시) !pip install --upgrade --force-reinstall pandas scikit-learn\n",
        "\n",
        "import os, glob, zipfile, warnings, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "# 혼합정밀 (GPU 있을 때만)\n",
        "MIXED = False\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    try:\n",
        "        from tensorflow.keras import mixed_precision\n",
        "        mixed_precision.set_global_policy('mixed_float16')\n",
        "        MIXED = True\n",
        "    except Exception:\n",
        "        MIXED = False\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 0-A) 데이터셋 준비 (ZIP 없어도 동작)\n",
        "#  - /content/dataset 안에 cup/ 폴더가 있으면 그대로 사용\n",
        "#  - 현재 작업 폴더에서 최신 ZIP 자동 탐색 (dataset(2).zip 등 포함)\n",
        "#  - 없으면 업로드 창 표시(Colab)\n",
        "# =========================================\n",
        "# =========================================\n",
        "# 0-A) 데이터셋 준비 (드라이브 ZIP 우선 사용)\n",
        "# =========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "EXTRACT_DIR = \"/content/dataset\"\n",
        "# 폴더로 이미 풀어둔 경우 👉 이 경로를 폴더로 지정하면 압축해제 생략\n",
        "CUSTOM_DATASET_DIR = None  # 예: \"/content/drive/MyDrive/dataset\"  (폴더일 때만)\n",
        "\n",
        "# 드라이브에 올린 ZIP 경로 (정확히 여기에 있다면 바로 사용)\n",
        "PREFERRED_ZIP = \"/content/drive/MyDrive/dataset.zip\"\n",
        "\n",
        "def ensure_dataset_dir():\n",
        "    # 1) 사용자가 폴더를 직접 지정한 경우 (ZIP 아님)\n",
        "    if CUSTOM_DATASET_DIR:\n",
        "        assert os.path.isdir(CUSTOM_DATASET_DIR), f\"경로가 폴더가 아닙니다: {CUSTOM_DATASET_DIR}\"\n",
        "        print(f\"직접 지정 폴더 사용: {CUSTOM_DATASET_DIR}\")\n",
        "        return CUSTOM_DATASET_DIR\n",
        "\n",
        "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "    # 이미 풀려 있으면 사용\n",
        "    if os.path.isdir(os.path.join(EXTRACT_DIR, \"cup\")):\n",
        "        print(\"기존 폴더 발견: 압축 해제 스킵\")\n",
        "        return EXTRACT_DIR\n",
        "\n",
        "    # 2) 드라이브 고정 경로 ZIP 우선\n",
        "    candidate_zips = []\n",
        "    if os.path.isfile(PREFERRED_ZIP):\n",
        "        candidate_zips.append(PREFERRED_ZIP)\n",
        "\n",
        "    # 3) 현재 작업 폴더의 ZIP 자동 탐색 (dataset(2).zip 등 포함)\n",
        "    candidate_zips += sorted(glob.glob(\"*.zip\"), key=os.path.getmtime, reverse=True)\n",
        "\n",
        "    # 4) 그래도 없으면 업로드 유도\n",
        "    if not candidate_zips:\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            print(\"📤 ZIP이 없어 업로드 창을 엽니다. dataset.zip을 선택하세요.\")\n",
        "            uploaded = files.upload()\n",
        "            candidate_zips = [os.path.join(os.getcwd(), k) for k in uploaded.keys()]\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(\"*.zip 파일이 없고 업로드도 되지 않았습니다.\") from e\n",
        "\n",
        "    zip_path = candidate_zips[0]\n",
        "    print(f\"사용 ZIP: {zip_path}\")\n",
        "\n",
        "    # 압축 테스트 및 해제\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "            bad = zf.testzip()\n",
        "            if bad:\n",
        "                raise zipfile.BadZipFile(f\"손상된 파일: {bad}\")\n",
        "            zf.extractall(EXTRACT_DIR)\n",
        "    except zipfile.BadZipFile as e:\n",
        "        raise RuntimeError(\n",
        "            f\"ZIP이 손상되었습니다: {e}\\n\"\n",
        "            f\"- 폴더를 다시 ZIP으로 압축해 재업로드하거나,\\n\"\n",
        "            f\"- Google Drive를 마운트해 폴더 경로를 직접 지정하세요.\"\n",
        "        )\n",
        "\n",
        "    # ZIP 내부에 'dataset/' 폴더가 한 겹 더 있으면 그 내부를 사용\n",
        "    inner = os.path.join(EXTRACT_DIR, \"dataset\")\n",
        "    if os.path.isdir(inner):\n",
        "        print(\"↪️ 내부 'dataset' 폴더 발견: 해당 경로 사용\")\n",
        "        return inner\n",
        "\n",
        "    return EXTRACT_DIR\n",
        "\n",
        "DATASET_DIR = ensure_dataset_dir()\n",
        "print(\"📁 DATASET_DIR:\", DATASET_DIR)\n",
        "\n",
        "# =========================================\n",
        "# 1) 라벨 규칙 (객체 6클래스 + 재질 3클래스)\n",
        "#  - 요청하신 매핑을 그대로 반영\n",
        "# =========================================\n",
        "OBJECT_CLASS_NAMES = ['cup_paper', 'cup_plastic', 'cupholder', 'lid', 'straw', 'none']\n",
        "OBJECT_CLASS_MAP   = {n:i for i,n in enumerate(OBJECT_CLASS_NAMES)}\n",
        "\n",
        "MATERIAL_CLASS_NAMES= ['paper', 'plastic', 'none']\n",
        "MATERIAL_CLASS_MAP  = {n:i for i,n in enumerate(MATERIAL_CLASS_NAMES)}\n",
        "\n",
        "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
        "\n",
        "def infer_labels_from_path(path: str):\n",
        "    \"\"\"\n",
        "    폴더 구조:\n",
        "      - cup/paper/*, cup/plastic/*\n",
        "      - cupholder/*\n",
        "      - lid/*\n",
        "      - straw/*\n",
        "      - none/*\n",
        "    재질 매핑:\n",
        "      cup_paper → paper\n",
        "      cup_plastic → plastic\n",
        "      cupholder → paper\n",
        "      lid → plastic\n",
        "      straw → plastic\n",
        "      none → none\n",
        "    \"\"\"\n",
        "    base = Path(DATASET_DIR).resolve()\n",
        "    rel  = Path(path).resolve().relative_to(base)\n",
        "    top  = rel.parts[0].lower()\n",
        "    sub  = rel.parts[1].lower() if len(rel.parts) > 1 else ''\n",
        "\n",
        "    if top == 'cup':\n",
        "        if sub == 'paper':\n",
        "            return ('cup_paper', 'paper')\n",
        "        elif sub == 'plastic':\n",
        "            return ('cup_plastic', 'plastic')\n",
        "        else:\n",
        "            # cup 바로 아래 이미지인 경우 파일명 힌트\n",
        "            name = Path(path).stem.lower()\n",
        "            if 'paper' in name:\n",
        "                return ('cup_paper', 'paper')\n",
        "            elif 'plastic' in name:\n",
        "                return ('cup_plastic', 'plastic')\n",
        "            raise ValueError(f\"[cup] 재질 하위폴더/힌트 불명확: {path}\")\n",
        "\n",
        "    elif top == 'cupholder':\n",
        "        return ('cupholder', 'paper')\n",
        "\n",
        "    elif top == 'lid':\n",
        "        return ('lid', 'plastic')\n",
        "\n",
        "    elif top == 'straw':\n",
        "        return ('straw', 'plastic')  # 전부 plastic 고정\n",
        "\n",
        "    elif top == 'none':\n",
        "        return ('none', 'none')\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"알 수 없는 최상위 폴더: {top} (path={path})\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 2) CSV 생성\n",
        "# =========================================\n",
        "image_paths, object_classes, materials = [], [], []\n",
        "\n",
        "for root, _, files in os.walk(DATASET_DIR):\n",
        "    for fname in files:\n",
        "        if fname.lower().endswith(IMG_EXTS):\n",
        "            fpath = os.path.join(root, fname)\n",
        "            try:\n",
        "                obj_cls, mat = infer_labels_from_path(fpath)\n",
        "            except Exception as e:\n",
        "                print(\"라벨 추론 스킵:\", e)\n",
        "                continue\n",
        "            image_paths.append(fpath)\n",
        "            object_classes.append(obj_cls)\n",
        "            materials.append(mat)\n",
        "\n",
        "if not image_paths:\n",
        "    raise RuntimeError(\"이미지를 찾지 못했습니다. 폴더 구조/확장자 확인 필요.\")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"image_path\": image_paths,\n",
        "    \"object_class\": object_classes,\n",
        "    \"material_type\": materials\n",
        "})\n",
        "\n",
        "print(\"원본 분포(객체):\\n\", df['object_class'].value_counts().sort_index())\n",
        "print(\"원본 분포(재질):\\n\", df['material_type'].value_counts().sort_index())\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 3) 균형화 (객체 클래스별 정확히 500장)\n",
        "# =========================================\n",
        "TARGET_PER_CLASS = 500\n",
        "balanced = []\n",
        "for cls in OBJECT_CLASS_NAMES:\n",
        "    g = df[df['object_class'] == cls]\n",
        "    n = len(g)\n",
        "    if n == 0:\n",
        "        raise RuntimeError(f\"'{cls}' 클래스 이미지가 없습니다.\")\n",
        "    if n == TARGET_PER_CLASS:\n",
        "        balanced.append(g)\n",
        "    elif n > TARGET_PER_CLASS:\n",
        "        balanced.append(g.sample(n=TARGET_PER_CLASS, replace=False, random_state=SEED))\n",
        "    else:\n",
        "        balanced.append(g.sample(n=TARGET_PER_CLASS, replace=True, random_state=SEED))\n",
        "\n",
        "df = pd.concat(balanced, ignore_index=True)\n",
        "\n",
        "# 라벨 인코딩\n",
        "df['object_label']   = df['object_class'].map(OBJECT_CLASS_MAP).astype(np.int32)\n",
        "df['material_label'] = df['material_type'].map(MATERIAL_CLASS_MAP).astype(np.int32)\n",
        "\n",
        "print(\"\\n균형 분포(객체):\\n\", df['object_class'].value_counts().sort_index())\n",
        "print(\"\\n균형 분포(재질):\\n\", df['material_type'].value_counts().sort_index())\n",
        "\n",
        "CSV_PATH = \"/content/labels.csv\"\n",
        "df.to_csv(CSV_PATH, index=False)\n",
        "print(f\"\\nlabels.csv 저장 완료: {CSV_PATH} (총 {len(df)}장)\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 4) 데이터 분할 (Train/Val/Test = 70/15/15, 객체 기준 stratify)\n",
        "# =========================================\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.30, random_state=SEED, stratify=df['object_label']\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.50, random_state=SEED, stratify=temp_df['object_label']\n",
        ")\n",
        "\n",
        "print(f\"\\nSplit -> Train:{len(train_df)}  Val:{len(val_df)}  Test:{len(test_df)}\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 5) tf.data 파이프라인\n",
        "# =========================================\n",
        "IMG_SIZE = (180, 180)\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "], name=\"augmentation\")\n",
        "\n",
        "def load_and_preprocess(path, obj_label, mat_label, augment=False):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, IMG_SIZE, method='bilinear')\n",
        "    img = preprocess_input(img)  # [-1,1]\n",
        "    if augment:\n",
        "        img = data_augmentation(img, training=True)\n",
        "    return img, {'object_output': obj_label, 'material_output': mat_label}\n",
        "\n",
        "def make_ds(dataframe, training=False):\n",
        "    paths = dataframe['image_path'].astype(str).values\n",
        "    obj   = dataframe['object_label'].astype(np.int32).values\n",
        "    mat   = dataframe['material_label'].astype(np.int32).values\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, obj, mat))\n",
        "    if training:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe), seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(lambda p,o,m: load_and_preprocess(p,o,m,augment=training), num_parallel_calls=AUTO)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
        "    # 검증/테스트는 순서 보장(평가 신뢰도 ↑)\n",
        "    if not training:\n",
        "        opts = tf.data.Options()\n",
        "        opts.experimental_deterministic = True\n",
        "        ds = ds.with_options(opts)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_ds(train_df, training=True)\n",
        "val_ds   = make_ds(val_df,   training=False)\n",
        "test_ds  = make_ds(test_df,  training=False)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 6) 모델 (MobileNetV2 멀티아웃풋)\n",
        "# =========================================\n",
        "def build_model():\n",
        "    base = MobileNetV2(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), include_top=False, weights='imagenet')\n",
        "    base.trainable = False\n",
        "\n",
        "    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    x = base(inp, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    dtype_out = 'float32' if MIXED else None\n",
        "    object_output   = layers.Dense(len(OBJECT_CLASS_NAMES),   activation='softmax', dtype=dtype_out, name='object_output')(x)\n",
        "    material_output = layers.Dense(len(MATERIAL_CLASS_NAMES), activation='softmax', dtype=dtype_out, name='material_output')(x)\n",
        "\n",
        "    model = models.Model(inp, {'object_output': object_output, 'material_output': material_output})\n",
        "    return model, base\n",
        "\n",
        "model, base = build_model()\n",
        "\n",
        "ckpt_path = \"/content/best_model.keras\"\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_object_output_accuracy',\n",
        "                                       mode='max', save_best_only=True, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=2, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# 1단계: Warm-up (베이스 고정)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss={'object_output': 'sparse_categorical_crossentropy',\n",
        "          'material_output': 'sparse_categorical_crossentropy'},\n",
        "    metrics={'object_output': 'accuracy', 'material_output': 'accuracy'}\n",
        ")\n",
        "history_warm = model.fit(train_ds, validation_data=val_ds, epochs=7, callbacks=callbacks, verbose=1)\n",
        "\n",
        "# 2단계: 상위 블록 파인튜닝 (BN 제외)\n",
        "N_TRAIN = 50\n",
        "for lyr in base.layers[-N_TRAIN:]:\n",
        "    if not isinstance(lyr, layers.BatchNormalization):\n",
        "        lyr.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(2e-4),\n",
        "    loss={'object_output': 'sparse_categorical_crossentropy',\n",
        "          'material_output': 'sparse_categorical_crossentropy'},\n",
        "    metrics={'object_output': 'accuracy', 'material_output': 'accuracy'}\n",
        ")\n",
        "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=12, callbacks=callbacks, verbose=1)\n",
        "\n",
        "FINAL_MODEL_PATH = \"/content/multilabel_model.keras\"\n",
        "model.save(FINAL_MODEL_PATH)\n",
        "print(\"모델 저장:\", FINAL_MODEL_PATH)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 7) 평가 (Val/Test) - 객체/재질\n",
        "#  - val/test는 순서를 고정했으므로 df와 1:1 대응\n",
        "# =========================================\n",
        "def evaluate_split(name, ds, df_ref):\n",
        "    preds = model.predict(ds, verbose=0)\n",
        "    y_obj_pred = preds['object_output'].argmax(axis=1)\n",
        "    y_mat_pred = preds['material_output'].argmax(axis=1)\n",
        "\n",
        "    y_obj_true = df_ref['object_label'].values\n",
        "    y_mat_true = df_ref['material_label'].values\n",
        "\n",
        "    print(f\"\\n=== [{name}] Object Report ===\")\n",
        "    print(classification_report(y_obj_true, y_obj_pred, target_names=OBJECT_CLASS_NAMES, digits=4))\n",
        "    print(\"Confusion Matrix (Object):\\n\", confusion_matrix(y_obj_true, y_obj_pred))\n",
        "\n",
        "    print(f\"\\n=== [{name}] Material Report ===\")\n",
        "    print(classification_report(y_mat_true, y_mat_pred, target_names=MATERIAL_CLASS_NAMES, digits=4))\n",
        "    print(\"Confusion Matrix (Material):\\n\", confusion_matrix(y_mat_true, y_mat_pred))\n",
        "\n",
        "evaluate_split(\"VAL\",  val_ds,  val_df)\n",
        "evaluate_split(\"TEST\", test_ds, test_df)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 8) 추론 유틸\n",
        "# =========================================\n",
        "from tensorflow.keras.preprocessing import image as kimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_image(img_path, none_threshold=0.55):\n",
        "    p = Path(img_path)\n",
        "    if not p.exists():\n",
        "        print(f\"파일 없음: {img_path}\"); return\n",
        "    img = kimage.load_img(img_path, target_size=IMG_SIZE)\n",
        "    arr = kimage.img_to_array(img)\n",
        "    x = tf.expand_dims(preprocess_input(arr), axis=0)\n",
        "\n",
        "    preds = model.predict(x, verbose=0)\n",
        "    obj_prob = preds['object_output'][0]\n",
        "    mat_prob = preds['material_output'][0]\n",
        "    o_idx = int(np.argmax(obj_prob))\n",
        "    m_idx = int(np.argmax(mat_prob))\n",
        "\n",
        "    # 낮은 확률이면 none으로 게이팅 (none 자체가 최고면 유지)\n",
        "    if obj_prob[o_idx] < none_threshold and OBJECT_CLASS_NAMES[o_idx] != 'none':\n",
        "        o_idx = OBJECT_CLASS_MAP['none']\n",
        "\n",
        "    print(f\"Object: {OBJECT_CLASS_NAMES[o_idx]} ({obj_prob[o_idx]*100:.2f}%)\")\n",
        "    print(f\"Material: {MATERIAL_CLASS_NAMES[m_idx]} ({mat_prob[m_idx]*100:.2f}%)\")\n",
        "\n",
        "def predict_and_show(img_path, none_threshold=0.55):\n",
        "    img = kimage.load_img(img_path, target_size=IMG_SIZE)\n",
        "    arr = kimage.img_to_array(img).astype(\"uint8\")\n",
        "    x = tf.expand_dims(preprocess_input(arr), axis=0)\n",
        "\n",
        "    preds = model.predict(x, verbose=0)\n",
        "    obj_prob = preds['object_output'][0]\n",
        "    mat_prob = preds['material_output'][0]\n",
        "    o_idx = int(np.argmax(obj_prob))\n",
        "    m_idx = int(np.argmax(mat_prob))\n",
        "    if obj_prob[o_idx] < none_threshold and OBJECT_CLASS_NAMES[o_idx] != 'none':\n",
        "        o_idx = OBJECT_CLASS_MAP['none']\n",
        "\n",
        "    plt.imshow(arr); plt.axis(\"off\")\n",
        "    plt.title(f\"{OBJECT_CLASS_NAMES[o_idx]} ({obj_prob[o_idx]*100:.1f}%)\\n\"\n",
        "              f\"{MATERIAL_CLASS_NAMES[m_idx]} ({mat_prob[m_idx]*100:.1f}%)\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KQW7U4h9HDem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3a83e7-986b-4547-e1ab-ce51fe036f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object_class  material_type\n",
            "cup_paper     paper            500\n",
            "cup_plastic   plastic          500\n",
            "cupholder     paper            500\n",
            "lid           plastic          500\n",
            "none          none             500\n",
            "straw         plastic          500\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/labels.csv\")\n",
        "print(df.groupby(['object_class','material_type']).size().sort_index())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}